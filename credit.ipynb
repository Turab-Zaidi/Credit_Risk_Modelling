{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sklearn.set_config(transform_output='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel('case_study1.xlsx')\n",
    "data2 = pd.read_excel('case_study2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and making of a proper dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In these datasets null values are marked as -99999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROSPECTID</th>\n",
       "      <th>Total_TL</th>\n",
       "      <th>Tot_Closed_TL</th>\n",
       "      <th>Tot_Active_TL</th>\n",
       "      <th>Total_TL_opened_L6M</th>\n",
       "      <th>Tot_TL_closed_L6M</th>\n",
       "      <th>pct_tl_open_L6M</th>\n",
       "      <th>pct_tl_closed_L6M</th>\n",
       "      <th>pct_active_tl</th>\n",
       "      <th>pct_closed_tl</th>\n",
       "      <th>...</th>\n",
       "      <th>CC_TL</th>\n",
       "      <th>Consumer_TL</th>\n",
       "      <th>Gold_TL</th>\n",
       "      <th>Home_TL</th>\n",
       "      <th>PL_TL</th>\n",
       "      <th>Secured_TL</th>\n",
       "      <th>Unsecured_TL</th>\n",
       "      <th>Other_TL</th>\n",
       "      <th>Age_Oldest_TL</th>\n",
       "      <th>Age_Newest_TL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PROSPECTID  Total_TL  Tot_Closed_TL  Tot_Active_TL  Total_TL_opened_L6M  \\\n",
       "0           1         5              4              1                    0   \n",
       "1           2         1              0              1                    0   \n",
       "2           3         8              0              8                    1   \n",
       "3           4         1              0              1                    1   \n",
       "4           5         3              2              1                    0   \n",
       "\n",
       "   Tot_TL_closed_L6M  pct_tl_open_L6M  pct_tl_closed_L6M  pct_active_tl  \\\n",
       "0                  0            0.000                0.0          0.200   \n",
       "1                  0            0.000                0.0          1.000   \n",
       "2                  0            0.125                0.0          1.000   \n",
       "3                  0            1.000                0.0          1.000   \n",
       "4                  0            0.000                0.0          0.333   \n",
       "\n",
       "   pct_closed_tl  ...  CC_TL  Consumer_TL  Gold_TL  Home_TL  PL_TL  \\\n",
       "0          0.800  ...      0            0        1        0      4   \n",
       "1          0.000  ...      0            1        0        0      0   \n",
       "2          0.000  ...      0            6        1        0      0   \n",
       "3          0.000  ...      0            0        0        0      0   \n",
       "4          0.667  ...      0            0        0        0      0   \n",
       "\n",
       "   Secured_TL  Unsecured_TL  Other_TL  Age_Oldest_TL  Age_Newest_TL  \n",
       "0           1             4         0             72             18  \n",
       "1           0             1         0              7              7  \n",
       "2           2             6         0             47              2  \n",
       "3           0             1         1              5              5  \n",
       "4           3             0         2            131             32  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51336, 26)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROSPECTID ----- 0\n",
      "Total_TL ----- 0\n",
      "Tot_Closed_TL ----- 0\n",
      "Tot_Active_TL ----- 0\n",
      "Total_TL_opened_L6M ----- 0\n",
      "Tot_TL_closed_L6M ----- 0\n",
      "pct_tl_open_L6M ----- 0\n",
      "pct_tl_closed_L6M ----- 0\n",
      "pct_active_tl ----- 0\n",
      "pct_closed_tl ----- 0\n",
      "Total_TL_opened_L12M ----- 0\n",
      "Tot_TL_closed_L12M ----- 0\n",
      "pct_tl_open_L12M ----- 0\n",
      "pct_tl_closed_L12M ----- 0\n",
      "Tot_Missed_Pmnt ----- 0\n",
      "Auto_TL ----- 0\n",
      "CC_TL ----- 0\n",
      "Consumer_TL ----- 0\n",
      "Gold_TL ----- 0\n",
      "Home_TL ----- 0\n",
      "PL_TL ----- 0\n",
      "Secured_TL ----- 0\n",
      "Unsecured_TL ----- 0\n",
      "Other_TL ----- 0\n",
      "Age_Oldest_TL ----- 40\n",
      "Age_Newest_TL ----- 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in data1.columns:\n",
    "    k = 0\n",
    "    for j in data1[i] :\n",
    "        if j == -99999:\n",
    "            k+=1\n",
    "    print(i,\"-----\",k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There are 40 null values in Age_Oldest_TL and Age_Newest_TL. So we will be dropping these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1[(data1['Age_Newest_TL'] != -99999) & (data1['Age_Oldest_TL'] != -99999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51296, 26)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROSPECTID</th>\n",
       "      <th>time_since_recent_payment</th>\n",
       "      <th>time_since_first_deliquency</th>\n",
       "      <th>time_since_recent_deliquency</th>\n",
       "      <th>num_times_delinquent</th>\n",
       "      <th>max_delinquency_level</th>\n",
       "      <th>max_recent_level_of_deliq</th>\n",
       "      <th>num_deliq_6mts</th>\n",
       "      <th>num_deliq_12mts</th>\n",
       "      <th>num_deliq_6_12mts</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_CC_enq_L6m_of_L12m</th>\n",
       "      <th>pct_PL_enq_L6m_of_ever</th>\n",
       "      <th>pct_CC_enq_L6m_of_ever</th>\n",
       "      <th>max_unsec_exposure_inPct</th>\n",
       "      <th>HL_Flag</th>\n",
       "      <th>GL_Flag</th>\n",
       "      <th>last_prod_enq2</th>\n",
       "      <th>first_prod_enq2</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Approved_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>549</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>696</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>685</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5741.667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>693</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>673</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>583</td>\n",
       "      <td>-99999</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>-99999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-99999.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AL</td>\n",
       "      <td>AL</td>\n",
       "      <td>753</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PROSPECTID  time_since_recent_payment  time_since_first_deliquency  \\\n",
       "0           1                        549                           35   \n",
       "1           2                         47                       -99999   \n",
       "2           3                        302                           11   \n",
       "3           4                     -99999                       -99999   \n",
       "4           5                        583                       -99999   \n",
       "\n",
       "   time_since_recent_deliquency  num_times_delinquent  max_delinquency_level  \\\n",
       "0                            15                    11                     29   \n",
       "1                        -99999                     0                 -99999   \n",
       "2                             3                     9                     25   \n",
       "3                        -99999                     0                 -99999   \n",
       "4                        -99999                     0                 -99999   \n",
       "\n",
       "   max_recent_level_of_deliq  num_deliq_6mts  num_deliq_12mts  \\\n",
       "0                         29               0                0   \n",
       "1                          0               0                0   \n",
       "2                         25               1                9   \n",
       "3                          0               0                0   \n",
       "4                          0               0                0   \n",
       "\n",
       "   num_deliq_6_12mts  ...  pct_CC_enq_L6m_of_L12m  pct_PL_enq_L6m_of_ever  \\\n",
       "0                  0  ...                     0.0                     0.0   \n",
       "1                  0  ...                     0.0                     0.0   \n",
       "2                  8  ...                     0.0                     0.0   \n",
       "3                  0  ...                     0.0                     0.0   \n",
       "4                  0  ...                     0.0                     0.0   \n",
       "\n",
       "   pct_CC_enq_L6m_of_ever  max_unsec_exposure_inPct  HL_Flag  GL_Flag  \\\n",
       "0                     0.0                    13.333        1        0   \n",
       "1                     0.0                     0.860        0        0   \n",
       "2                     0.0                  5741.667        1        0   \n",
       "3                     0.0                     9.900        0        0   \n",
       "4                     0.0                -99999.000        0        0   \n",
       "\n",
       "   last_prod_enq2  first_prod_enq2  Credit_Score  Approved_Flag  \n",
       "0              PL               PL           696             P2  \n",
       "1    ConsumerLoan     ConsumerLoan           685             P2  \n",
       "2    ConsumerLoan           others           693             P2  \n",
       "3          others           others           673             P2  \n",
       "4              AL               AL           753             P1  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51336, 62)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROSPECTID ----- 0\n",
      "time_since_recent_payment ----- 4291\n",
      "time_since_first_deliquency ----- 35949\n",
      "time_since_recent_deliquency ----- 35949\n",
      "num_times_delinquent ----- 0\n",
      "max_delinquency_level ----- 35949\n",
      "max_recent_level_of_deliq ----- 0\n",
      "num_deliq_6mts ----- 0\n",
      "num_deliq_12mts ----- 0\n",
      "num_deliq_6_12mts ----- 0\n",
      "max_deliq_6mts ----- 12890\n",
      "max_deliq_12mts ----- 10832\n",
      "num_times_30p_dpd ----- 0\n",
      "num_times_60p_dpd ----- 0\n",
      "num_std ----- 0\n",
      "num_std_6mts ----- 0\n",
      "num_std_12mts ----- 0\n",
      "num_sub ----- 0\n",
      "num_sub_6mts ----- 0\n",
      "num_sub_12mts ----- 0\n",
      "num_dbt ----- 0\n",
      "num_dbt_6mts ----- 0\n",
      "num_dbt_12mts ----- 0\n",
      "num_lss ----- 0\n",
      "num_lss_6mts ----- 0\n",
      "num_lss_12mts ----- 0\n",
      "recent_level_of_deliq ----- 0\n",
      "tot_enq ----- 6321\n",
      "CC_enq ----- 6321\n",
      "CC_enq_L6m ----- 6321\n",
      "CC_enq_L12m ----- 6321\n",
      "PL_enq ----- 6321\n",
      "PL_enq_L6m ----- 6321\n",
      "PL_enq_L12m ----- 6321\n",
      "time_since_recent_enq ----- 6321\n",
      "enq_L12m ----- 6321\n",
      "enq_L6m ----- 6321\n",
      "enq_L3m ----- 6321\n",
      "MARITALSTATUS ----- 0\n",
      "EDUCATION ----- 0\n",
      "AGE ----- 0\n",
      "GENDER ----- 0\n",
      "NETMONTHLYINCOME ----- 0\n",
      "Time_With_Curr_Empr ----- 0\n",
      "pct_of_active_TLs_ever ----- 0\n",
      "pct_opened_TLs_L6m_of_L12m ----- 0\n",
      "pct_currentBal_all_TL ----- 72\n",
      "CC_utilization ----- 47636\n",
      "CC_Flag ----- 0\n",
      "PL_utilization ----- 44435\n",
      "PL_Flag ----- 0\n",
      "pct_PL_enq_L6m_of_L12m ----- 0\n",
      "pct_CC_enq_L6m_of_L12m ----- 0\n",
      "pct_PL_enq_L6m_of_ever ----- 0\n",
      "pct_CC_enq_L6m_of_ever ----- 0\n",
      "max_unsec_exposure_inPct ----- 23178\n",
      "HL_Flag ----- 0\n",
      "GL_Flag ----- 0\n",
      "last_prod_enq2 ----- 0\n",
      "first_prod_enq2 ----- 0\n",
      "Credit_Score ----- 0\n",
      "Approved_Flag ----- 0\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "for i in data2.columns:\n",
    "    k = 0\n",
    "    for j in data2[i] :\n",
    "        if j == -99999:\n",
    "            k+=1\n",
    "    print(i,\"-----\",k)\n",
    "    dict[i] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will drop the columns with null values more than 10k and drop the rows in case the value is less than 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column -  time_since_first_deliquency  dropped.\n",
      "Column -  time_since_recent_deliquency  dropped.\n",
      "Column -  max_delinquency_level  dropped.\n",
      "Column -  max_deliq_6mts  dropped.\n",
      "Column -  max_deliq_12mts  dropped.\n",
      "Column -  CC_utilization  dropped.\n",
      "Column -  PL_utilization  dropped.\n",
      "Column -  max_unsec_exposure_inPct  dropped.\n"
     ]
    }
   ],
   "source": [
    "for key,values in dict.items():\n",
    "    if values >= 10000:\n",
    "        data2.drop([key],inplace=True,axis=1)\n",
    "        print(\"Column - \",key,\" dropped.\")\n",
    "    elif values>0:\n",
    "        data2 = data2[data2[key] != -99999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42066, 54)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will merge the dataset on PROSPECTID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data1,data2,on='PROSPECTID',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROSPECTID</th>\n",
       "      <th>Total_TL</th>\n",
       "      <th>Tot_Closed_TL</th>\n",
       "      <th>Tot_Active_TL</th>\n",
       "      <th>Total_TL_opened_L6M</th>\n",
       "      <th>Tot_TL_closed_L6M</th>\n",
       "      <th>pct_tl_open_L6M</th>\n",
       "      <th>pct_tl_closed_L6M</th>\n",
       "      <th>pct_active_tl</th>\n",
       "      <th>pct_closed_tl</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_PL_enq_L6m_of_L12m</th>\n",
       "      <th>pct_CC_enq_L6m_of_L12m</th>\n",
       "      <th>pct_PL_enq_L6m_of_ever</th>\n",
       "      <th>pct_CC_enq_L6m_of_ever</th>\n",
       "      <th>HL_Flag</th>\n",
       "      <th>GL_Flag</th>\n",
       "      <th>last_prod_enq2</th>\n",
       "      <th>first_prod_enq2</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Approved_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>696</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>685</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>693</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AL</td>\n",
       "      <td>AL</td>\n",
       "      <td>753</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>PL</td>\n",
       "      <td>668</td>\n",
       "      <td>P3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PROSPECTID  Total_TL  Tot_Closed_TL  Tot_Active_TL  Total_TL_opened_L6M  \\\n",
       "0           1         5              4              1                    0   \n",
       "1           2         1              0              1                    0   \n",
       "2           3         8              0              8                    1   \n",
       "3           5         3              2              1                    0   \n",
       "4           6         6              5              1                    0   \n",
       "\n",
       "   Tot_TL_closed_L6M  pct_tl_open_L6M  pct_tl_closed_L6M  pct_active_tl  \\\n",
       "0                  0            0.000                0.0          0.200   \n",
       "1                  0            0.000                0.0          1.000   \n",
       "2                  0            0.125                0.0          1.000   \n",
       "3                  0            0.000                0.0          0.333   \n",
       "4                  0            0.000                0.0          0.167   \n",
       "\n",
       "   pct_closed_tl  ...  pct_PL_enq_L6m_of_L12m  pct_CC_enq_L6m_of_L12m  \\\n",
       "0          0.800  ...                     0.0                     0.0   \n",
       "1          0.000  ...                     0.0                     0.0   \n",
       "2          0.000  ...                     0.0                     0.0   \n",
       "3          0.667  ...                     0.0                     0.0   \n",
       "4          0.833  ...                     1.0                     0.0   \n",
       "\n",
       "   pct_PL_enq_L6m_of_ever  pct_CC_enq_L6m_of_ever  HL_Flag  GL_Flag  \\\n",
       "0                   0.000                     0.0        1        0   \n",
       "1                   0.000                     0.0        0        0   \n",
       "2                   0.000                     0.0        1        0   \n",
       "3                   0.000                     0.0        0        0   \n",
       "4                   0.429                     0.0        1        0   \n",
       "\n",
       "   last_prod_enq2  first_prod_enq2  Credit_Score  Approved_Flag  \n",
       "0              PL               PL           696             P2  \n",
       "1    ConsumerLoan     ConsumerLoan           685             P2  \n",
       "2    ConsumerLoan           others           693             P2  \n",
       "3              AL               AL           753             P1  \n",
       "4    ConsumerLoan               PL           668             P3  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42064, 79)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chisquare to find the association of categorical variable  with our dependent variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_column = []\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == 'object' and i != \"Approved_Flag\":\n",
    "        cat_column.append(i)\n",
    "cat_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARITALSTATUS  -----  3.578180861038862e-233\n",
      "EDUCATION  -----  2.6942265249737532e-30\n",
      "GENDER  -----  1.907936100186563e-05\n",
      "last_prod_enq2  -----  0.0\n",
      "first_prod_enq2  -----  7.84997610555419e-287\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "from scipy.stats import chi2_contingency\n",
    "for i in cat_column:\n",
    "    _, pval, _, _ = chi2_contingency(pd.crosstab(data[i],data['Approved_Flag']))\n",
    "    print(i,\" ----- \", pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All the pvalue are less than the signifance level alpha therefore we won't be dropping any of them since they are strongly associated with our dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will be checking our data for multicollinearity. For that we will be calculating VIF for our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column = []\n",
    "for i in data.columns:\n",
    "    if data[i].dtype != 'object' and i != \"PROSPECTID\":\n",
    "        num_column.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vif_data = data[num_column]\n",
    "total_no_of_columns = vif_data.shape[1]\n",
    "columns_to_be_kept = []\n",
    "column_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_TL --- inf\n",
      "Tot_Closed_TL --- inf\n",
      "Tot_Active_TL --- 11.320180023967996\n",
      "Total_TL_opened_L6M --- 8.363698035000336\n",
      "Tot_TL_closed_L6M --- 6.520647877790928\n",
      "pct_tl_open_L6M --- 5.149501618212625\n",
      "pct_tl_closed_L6M --- 2.611111040579735\n",
      "pct_active_tl --- inf\n",
      "pct_closed_tl --- 1788.7926256209232\n",
      "Total_TL_opened_L12M --- 8.601028256477228\n",
      "Tot_TL_closed_L12M --- 3.8328007921530785\n",
      "pct_tl_open_L12M --- 6.099653381646739\n",
      "pct_tl_closed_L12M --- 5.581352009642762\n",
      "Tot_Missed_Pmnt --- 1.985584353098778\n",
      "Auto_TL --- inf\n",
      "CC_TL --- 4.809538302819343\n",
      "Consumer_TL --- 23.270628983464636\n",
      "Gold_TL --- 30.595522588100053\n",
      "Home_TL --- 4.3843464059655854\n",
      "PL_TL --- 3.064658415523423\n",
      "Secured_TL --- 2.898639771299253\n",
      "Unsecured_TL --- 4.377876915347324\n",
      "Other_TL --- 2.207853583695844\n",
      "Age_Oldest_TL --- 4.916914200506864\n",
      "Age_Newest_TL --- 5.214702030064725\n",
      "time_since_recent_payment --- 3.3861625024231476\n",
      "num_times_delinquent --- 7.840583309478997\n",
      "max_recent_level_of_deliq --- 5.255034641721438\n",
      "num_deliq_6mts --- inf\n",
      "num_deliq_12mts --- 7.380634506427232\n",
      "num_deliq_6_12mts --- 1.4210050015175733\n",
      "num_times_30p_dpd --- 8.083255010190316\n",
      "num_times_60p_dpd --- 1.6241227524040112\n",
      "num_std --- 7.257811920140003\n",
      "num_std_6mts --- 15.59624383268298\n",
      "num_std_12mts --- 1.825857047132431\n",
      "num_sub --- 1.5080839450032664\n",
      "num_sub_6mts --- 2.172088834824577\n",
      "num_sub_12mts --- 2.62339755352723\n",
      "num_dbt --- 2.2959970812106176\n",
      "num_dbt_6mts --- 7.360578319196439\n",
      "num_dbt_12mts --- 2.1602387773102554\n",
      "num_lss --- 2.8686288267891458\n",
      "num_lss_6mts --- 6.458218003637277\n",
      "num_lss_12mts --- 2.8474118865638265\n",
      "recent_level_of_deliq --- 4.753198156284083\n",
      "tot_enq --- 16.22735475594825\n",
      "CC_enq --- 6.424377256363877\n",
      "CC_enq_L6m --- 8.887080381808687\n",
      "CC_enq_L12m --- 2.3804746142952653\n",
      "PL_enq --- 8.609513476514548\n",
      "PL_enq_L6m --- 13.06755093547673\n",
      "PL_enq_L12m --- 3.500040056654654\n",
      "time_since_recent_enq --- 1.9087955874813773\n",
      "enq_L12m --- 17.006562234161628\n",
      "enq_L6m --- 10.730485153719197\n",
      "enq_L3m --- 2.3538497522950275\n",
      "AGE --- 22.104855915136433\n",
      "NETMONTHLYINCOME --- 2.7971639638512906\n",
      "Time_With_Curr_Empr --- 3.4241712032176985\n",
      "pct_of_active_TLs_ever --- 10.175021454450935\n",
      "pct_opened_TLs_L6m_of_L12m --- 6.408710354561301\n",
      "pct_currentBal_all_TL --- 1.001151196262561\n",
      "CC_Flag --- 3.069197305397274\n",
      "PL_Flag --- 2.8091261600643715\n",
      "pct_PL_enq_L6m_of_L12m --- 20.249538381980678\n",
      "pct_CC_enq_L6m_of_L12m --- 15.864576541593774\n",
      "pct_PL_enq_L6m_of_ever --- 1.8331649740532172\n",
      "pct_CC_enq_L6m_of_ever --- 1.5680839909542037\n",
      "HL_Flag --- 1.9307572353811682\n",
      "GL_Flag --- 4.331265056645247\n",
      "Credit_Score --- 9.390334396150173\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "for i in range (0,total_no_of_columns):\n",
    "    \n",
    "    vif_value = variance_inflation_factor(vif_data, column_index)\n",
    "    print (num_column[i],'---',vif_value)\n",
    "    \n",
    "    \n",
    "    if vif_value <= 6:\n",
    "        columns_to_be_kept.append( num_column[i] )\n",
    "        column_index = column_index+1\n",
    "    \n",
    "    else:\n",
    "        vif_data = vif_data.drop([ num_column[i] ] , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we evaluate the association between  these numeric columns and the dependent variable using ANOVA tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pct_tl_open_L6M',\n",
       " 'pct_tl_closed_L6M',\n",
       " 'Tot_TL_closed_L12M',\n",
       " 'pct_tl_closed_L12M',\n",
       " 'Tot_Missed_Pmnt',\n",
       " 'CC_TL',\n",
       " 'Home_TL',\n",
       " 'PL_TL',\n",
       " 'Secured_TL',\n",
       " 'Unsecured_TL',\n",
       " 'Other_TL',\n",
       " 'Age_Oldest_TL',\n",
       " 'Age_Newest_TL',\n",
       " 'time_since_recent_payment',\n",
       " 'max_recent_level_of_deliq',\n",
       " 'num_deliq_6_12mts',\n",
       " 'num_times_60p_dpd',\n",
       " 'num_std_12mts',\n",
       " 'num_sub',\n",
       " 'num_sub_6mts',\n",
       " 'num_sub_12mts',\n",
       " 'num_dbt',\n",
       " 'num_dbt_12mts',\n",
       " 'num_lss',\n",
       " 'recent_level_of_deliq',\n",
       " 'CC_enq_L12m',\n",
       " 'PL_enq_L12m',\n",
       " 'time_since_recent_enq',\n",
       " 'enq_L3m',\n",
       " 'NETMONTHLYINCOME',\n",
       " 'Time_With_Curr_Empr',\n",
       " 'CC_Flag',\n",
       " 'PL_Flag',\n",
       " 'pct_PL_enq_L6m_of_ever',\n",
       " 'pct_CC_enq_L6m_of_ever',\n",
       " 'HL_Flag',\n",
       " 'GL_Flag']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "columns_to_be_kept_after_anova = []\n",
    "\n",
    "for i in columns_to_be_kept:\n",
    "    a = list(data[i])  \n",
    "    b = list(data['Approved_Flag'])  \n",
    "    \n",
    "    group_P1 = [value for value, group in zip(a, b) if group == 'P1']\n",
    "    group_P2 = [value for value, group in zip(a, b) if group == 'P2']\n",
    "    group_P3 = [value for value, group in zip(a, b) if group == 'P3']\n",
    "    group_P4 = [value for value, group in zip(a, b) if group == 'P4']\n",
    "\n",
    "\n",
    "    f_statistic, p_value = f_oneway(group_P1, group_P2, group_P3, group_P4)\n",
    "\n",
    "    if p_value <= 0.05:\n",
    "        columns_to_be_kept_after_anova.append(i)\n",
    "columns_to_be_kept_after_anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = columns_to_be_kept_after_anova + cat_column + ['Approved_Flag']\n",
    "final_dataset = data[all_columns ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = final_dataset.drop(['Approved_Flag'],axis=1)\n",
    "y = final_dataset['Approved_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33651, 42), (33651,))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test =  train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_column = []\n",
    "cat_column = []\n",
    "for i in x_train.columns:\n",
    "    if x_train[i].dtype == 'object':\n",
    "        cat_column.append(i)\n",
    "    else:\n",
    "        numeric_column.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARITALSTATUS\n",
      "\n",
      " ['Married' 'Single']\n",
      "\n",
      " ----------------------------------------------------------------------------\n",
      "EDUCATION\n",
      "\n",
      " ['GRADUATE' '12TH' 'POST-GRADUATE' 'UNDER GRADUATE' 'SSC' 'OTHERS'\n",
      " 'PROFESSIONAL']\n",
      "\n",
      " ----------------------------------------------------------------------------\n",
      "GENDER\n",
      "\n",
      " ['M' 'F']\n",
      "\n",
      " ----------------------------------------------------------------------------\n",
      "last_prod_enq2\n",
      "\n",
      " ['ConsumerLoan' 'HL' 'PL' 'others' 'CC' 'AL']\n",
      "\n",
      " ----------------------------------------------------------------------------\n",
      "first_prod_enq2\n",
      "\n",
      " ['AL' 'HL' 'PL' 'ConsumerLoan' 'others' 'CC']\n",
      "\n",
      " ----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in cat_column:\n",
    "    print(i)\n",
    "    print('\\n',x_train[i].unique())\n",
    "    print('\\n','-------------------'*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Among all the categorical variables, only 'EDUCATION' exhibits an ordinal relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mapping = {\n",
    "    '12TH': 1,\n",
    "    'UNDER GRADUATE': 2,\n",
    "    'GRADUATE' : 3,  \n",
    "    'PROFESSIONAL': 3,\n",
    "    'POST-GRADUATE' : 4,\n",
    "    'SSC' : 3,\n",
    "    'OTHERS' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['EDUCATION'] = x_train['EDUCATION'].map(custom_mapping)\n",
    "x_test['EDUCATION'] = x_test['EDUCATION'].map(custom_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_column.remove('EDUCATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer = Pipeline(steps=[\n",
    "\t(\"encoder\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"))\n",
    "])\n",
    "num_transformer = Pipeline(steps=[\n",
    "\t(\"scaler\", StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "\t(\"num\", num_transformer, numeric_column),\n",
    "\t(\"cat\", cat_transformer, cat_column)],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "algorithms = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=10),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=10),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=10),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ------  Accuracy :  0.7500297159158446    F1 Score :  0.7099010074817613\n",
      "Decision Tree ------  Accuracy :  0.6987994769998811    F1 Score :  0.700000658948072\n",
      "Random Forest ------  Accuracy :  0.7386188042315465    F1 Score :  0.7168471821819075\n",
      "Support Vector Machine ------  Accuracy :  0.7594199453227148    F1 Score :  0.7300404216184163\n",
      "K-Nearest Neighbors ------  Accuracy :  0.6987994769998811    F1 Score :  0.670524201637437\n",
      "Naive Bayes ------  Accuracy :  0.4696303340068941    F1 Score :  0.49338798557014096\n",
      "Gradient Boosting ------  Accuracy :  0.7230476643290146    F1 Score :  0.6585835599675984\n",
      "AdaBoost ------  Accuracy :  0.7129442529418757    F1 Score :  0.6672985765364557\n",
      "XGBoost ------  Accuracy :  0.7719006299774159    F1 Score :  0.7499260301663279\n"
     ]
    }
   ],
   "source": [
    "for name,algorithm in algorithms.items():\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', algorithm)\n",
    "    ])\n",
    "\n",
    "    if isinstance(algorithm, XGBClassifier):\n",
    "        encoder = LabelEncoder()\n",
    "        y_train_preprocessed = encoder.fit_transform(y_train)\n",
    "        y_test_preprocessed = encoder.transform(y_test)\n",
    "        model.fit(x_train, y_train_preprocessed)   \n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test_preprocessed, y_pred)\n",
    "        f1 = f1_score(y_test_preprocessed, y_pred, average='weighted')\n",
    "\n",
    "    else:  \n",
    "        y_train_preprocessed = y_train\n",
    "        model.fit(x_train, y_train_preprocessed)   \n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    \n",
    "    \n",
    "    print(name,\"------\",' Accuracy : ', accuracy, '   F1 Score : ', f1)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly xgboost outperforms all the other models therefore we will chose xgboost as our model and will perform hypertuning on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination :  1  Training_acc :  0.7151050488841342  Test_acc :  0.71330084393201\n",
      "Combination :  2  Training_acc :  0.7225639654096461  Test_acc :  0.721145845714965\n",
      "Combination :  3  Training_acc :  0.7273186532346736  Test_acc :  0.7261381195768454\n",
      "Combination :  4  Training_acc :  0.7348072865590919  Test_acc :  0.7338642576964222\n",
      "Combination :  5  Training_acc :  0.7492199340287065  Test_acc :  0.7401640318554618\n",
      "Combination :  6  Training_acc :  0.7578378057115688  Test_acc :  0.7468203970046356\n",
      "Combination :  7  Training_acc :  0.7629193783245669  Test_acc :  0.752763580173541\n",
      "Combination :  8  Training_acc :  0.7688033045080384  Test_acc :  0.757399263045287\n",
      "Combination :  9  Training_acc :  0.7803631392826365  Test_acc :  0.756210626411506\n",
      "Combination :  10  Training_acc :  0.7879706398026805  Test_acc :  0.7623915369071674\n",
      "Combination :  11  Training_acc :  0.7937951323883391  Test_acc :  0.7639367645310828\n",
      "Combination :  12  Training_acc :  0.7990847225936822  Test_acc :  0.7683347200760727\n",
      "Combination :  13  Training_acc :  0.8183412082850435  Test_acc :  0.7621538095804112\n",
      "Combination :  14  Training_acc :  0.8282666191197884  Test_acc :  0.7663140377986449\n",
      "Combination :  15  Training_acc :  0.8379840123621883  Test_acc :  0.7670272197789136\n",
      "Combination :  16  Training_acc :  0.8463047160559865  Test_acc :  0.768691311066207\n",
      "Combination :  17  Training_acc :  0.7386110368191139  Test_acc :  0.7364792582907406\n",
      "Combination :  18  Training_acc :  0.7606014680098659  Test_acc :  0.7544276714608344\n",
      "Combination :  19  Training_acc :  0.7725179043713412  Test_acc :  0.7640556281944609\n",
      "Combination :  20  Training_acc :  0.7777183441799649  Test_acc :  0.7694044930464757\n",
      "Combination :  21  Training_acc :  0.7734688419363466  Test_acc :  0.7615594912635207\n",
      "Combination :  22  Training_acc :  0.7898130813348786  Test_acc :  0.7728515392844407\n",
      "Combination :  23  Training_acc :  0.7974800154527354  Test_acc :  0.7790324497801022\n",
      "Combination :  24  Training_acc :  0.8052658167662179  Test_acc :  0.7790324497801022\n",
      "Combination :  25  Training_acc :  0.8038096936198033  Test_acc :  0.7695233567098538\n",
      "Combination :  26  Training_acc :  0.8247897536477371  Test_acc :  0.7755854035421371\n",
      "Combination :  27  Training_acc :  0.8396184363020416  Test_acc :  0.7773683584928087\n",
      "Combination :  28  Training_acc :  0.8526938278208671  Test_acc :  0.7776060858195649\n",
      "Combination :  29  Training_acc :  0.8520994918427387  Test_acc :  0.7676215380958041\n",
      "Combination :  30  Training_acc :  0.8836290154824522  Test_acc :  0.7739213122548437\n",
      "Combination :  31  Training_acc :  0.9020831476033402  Test_acc :  0.7748722215618685\n",
      "Combination :  32  Training_acc :  0.9171198478499896  Test_acc :  0.7745156305717342\n",
      "Combination :  33  Training_acc :  0.7590859112656385  Test_acc :  0.7524069891834066\n",
      "Combination :  34  Training_acc :  0.7782532465602805  Test_acc :  0.7699988113633662\n",
      "Combination :  35  Training_acc :  0.7849395263142255  Test_acc :  0.7764174491857839\n",
      "Combination :  36  Training_acc :  0.7911503372856676  Test_acc :  0.7791513134434803\n",
      "Combination :  37  Training_acc :  0.7890107277644052  Test_acc :  0.7743967669083561\n",
      "Combination :  38  Training_acc :  0.8055629847552822  Test_acc :  0.7764174491857839\n",
      "Combination :  39  Training_acc :  0.8173605539211316  Test_acc :  0.7791513134434803\n",
      "Combination :  40  Training_acc :  0.8288609550979169  Test_acc :  0.7812908593842862\n",
      "Combination :  41  Training_acc :  0.8222638257406912  Test_acc :  0.7759419945322715\n",
      "Combination :  42  Training_acc :  0.8519509078482066  Test_acc :  0.7779626768096993\n",
      "Combination :  43  Training_acc :  0.8740899230334909  Test_acc :  0.7776060858195649\n",
      "Combination :  44  Training_acc :  0.8923657543609402  Test_acc :  0.7773683584928087\n",
      "Combination :  45  Training_acc :  0.8817568571513477  Test_acc :  0.7721383573041721\n",
      "Combination :  46  Training_acc :  0.9157528751002942  Test_acc :  0.7727326756210626\n",
      "Combination :  47  Training_acc :  0.9408635701762206  Test_acc :  0.7727326756210626\n",
      "Combination :  48  Training_acc :  0.957802145552881  Test_acc :  0.7745156305717342\n",
      "Combination :  49  Training_acc :  0.7775994769843393  Test_acc :  0.7717817663140378\n",
      "Combination :  50  Training_acc :  0.7913286380791061  Test_acc :  0.7796267680969927\n",
      "Combination :  51  Training_acc :  0.7994116073816528  Test_acc :  0.7817663140377986\n",
      "Combination :  52  Training_acc :  0.8047606311848088  Test_acc :  0.7820040413645548\n",
      "Combination :  53  Training_acc :  0.8054738343585629  Test_acc :  0.7790324497801022\n",
      "Combination :  54  Training_acc :  0.828236902320882  Test_acc :  0.7809342683941519\n",
      "Combination :  55  Training_acc :  0.8473448040177112  Test_acc :  0.7796267680969927\n",
      "Combination :  56  Training_acc :  0.864521113785623  Test_acc :  0.7793890407702365\n",
      "Combination :  57  Training_acc :  0.8514457222667974  Test_acc :  0.7747533578984904\n",
      "Combination :  58  Training_acc :  0.8952482838548631  Test_acc :  0.7773683584928087\n",
      "Combination :  59  Training_acc :  0.9227066060443969  Test_acc :  0.7727326756210626\n",
      "Combination :  60  Training_acc :  0.9449942052242133  Test_acc :  0.774277903244978\n",
      "Combination :  61  Training_acc :  0.9163472110784227  Test_acc :  0.7741590395815999\n",
      "Combination :  62  Training_acc :  0.9569700751835012  Test_acc :  0.7715440389872816\n",
      "Combination :  63  Training_acc :  0.9805057799173873  Test_acc :  0.7727326756210626\n",
      "Combination :  64  Training_acc :  0.9926599506701138  Test_acc :  0.7698799476999881\n",
      "Combination :  65  Training_acc :  0.7858013134825117  Test_acc :  0.7773683584928087\n",
      "Combination :  66  Training_acc :  0.7991144393925886  Test_acc :  0.7824794960180673\n",
      "Combination :  67  Training_acc :  0.8069299575049775  Test_acc :  0.7816474503744205\n",
      "Combination :  68  Training_acc :  0.8140025556447059  Test_acc :  0.7809342683941519\n",
      "Combination :  69  Training_acc :  0.8160827315681555  Test_acc :  0.7793890407702365\n",
      "Combination :  70  Training_acc :  0.8494844135389736  Test_acc :  0.7803399500772614\n",
      "Combination :  71  Training_acc :  0.8748922766039642  Test_acc :  0.7760608581956496\n",
      "Combination :  72  Training_acc :  0.8920091527740632  Test_acc :  0.775466539878759\n",
      "Combination :  73  Training_acc :  0.8729904014739532  Test_acc :  0.7759419945322715\n",
      "Combination :  74  Training_acc :  0.9241330123919052  Test_acc :  0.7721383573041721\n",
      "Combination :  75  Training_acc :  0.9544144304775489  Test_acc :  0.7738024485914656\n",
      "Combination :  76  Training_acc :  0.9742058185492258  Test_acc :  0.770949720670391\n",
      "Combination :  77  Training_acc :  0.94377581646905  Test_acc :  0.7752288125520028\n",
      "Combination :  78  Training_acc :  0.9779798520103414  Test_acc :  0.7739213122548437\n",
      "Combination :  79  Training_acc :  0.994710409794657  Test_acc :  0.7684535837394508\n",
      "Combination :  80  Training_acc :  0.9991679296306202  Test_acc :  0.7677404017591822\n",
      "Combination :  81  Training_acc :  0.718611631155092  Test_acc :  0.715559253536194\n",
      "Combination :  82  Training_acc :  0.7247927253276277  Test_acc :  0.7229288006656365\n",
      "Combination :  83  Training_acc :  0.7356987905262845  Test_acc :  0.7343397123499347\n",
      "Combination :  84  Training_acc :  0.7398294255742771  Test_acc :  0.7384999405681683\n",
      "Combination :  85  Training_acc :  0.7542717898427981  Test_acc :  0.7443242600736955\n",
      "Combination :  86  Training_acc :  0.7629490951234733  Test_acc :  0.7526447165101628\n",
      "Combination :  87  Training_acc :  0.7689221717036641  Test_acc :  0.7572803993819089\n",
      "Combination :  88  Training_acc :  0.7743306291046328  Test_acc :  0.7626292642339237\n",
      "Combination :  89  Training_acc :  0.7907343021009777  Test_acc :  0.7634613098775704\n",
      "Combination :  90  Training_acc :  0.7956375739205372  Test_acc :  0.7677404017591822\n",
      "Combination :  91  Training_acc :  0.7996790585718107  Test_acc :  0.7692856293830976\n",
      "Combination :  92  Training_acc :  0.8038394104187097  Test_acc :  0.7717817663140378\n",
      "Combination :  93  Training_acc :  0.8268402127722801  Test_acc :  0.7683347200760727\n",
      "Combination :  94  Training_acc :  0.835071766069359  Test_acc :  0.7699988113633662\n",
      "Combination :  95  Training_acc :  0.8423820986003387  Test_acc :  0.7724949482943064\n",
      "Combination :  96  Training_acc :  0.8484443255772488  Test_acc :  0.7733269939379531\n",
      "Combination :  97  Training_acc :  0.7447029805949303  Test_acc :  0.7402828955188399\n",
      "Combination :  98  Training_acc :  0.7625627767376898  Test_acc :  0.7575181267086651\n",
      "Combination :  99  Training_acc :  0.771240082018365  Test_acc :  0.7659574468085106\n",
      "Combination :  100  Training_acc :  0.778639564946064  Test_acc :  0.7719006299774159\n",
      "Combination :  101  Training_acc :  0.7768862738105852  Test_acc :  0.766432901462023\n",
      "Combination :  102  Training_acc :  0.7913880716769189  Test_acc :  0.7749910852252466\n",
      "Combination :  103  Training_acc :  0.8003031113488455  Test_acc :  0.7776060858195649\n",
      "Combination :  104  Training_acc :  0.8074945766841995  Test_acc :  0.7790324497801022\n",
      "Combination :  105  Training_acc :  0.8066922231137262  Test_acc :  0.7726138119576845\n",
      "Combination :  106  Training_acc :  0.8250274880389884  Test_acc :  0.7766551765125401\n",
      "Combination :  107  Training_acc :  0.839083533921726  Test_acc :  0.7784381314632117\n",
      "Combination :  108  Training_acc :  0.8527235446197735  Test_acc :  0.7786758587899679\n",
      "Combination :  109  Training_acc :  0.8548928709399424  Test_acc :  0.7752288125520028\n",
      "Combination :  110  Training_acc :  0.8820837419393183  Test_acc :  0.7767740401759182\n",
      "Combination :  111  Training_acc :  0.901221360435054  Test_acc :  0.7762985855224058\n",
      "Combination :  112  Training_acc :  0.9164957950729548  Test_acc :  0.7782004041364555\n",
      "Combination :  113  Training_acc :  0.763067962319099  Test_acc :  0.756210626411506\n",
      "Combination :  114  Training_acc :  0.7794716353154438  Test_acc :  0.772019493640794\n",
      "Combination :  115  Training_acc :  0.7875843214168969  Test_acc :  0.7785569951265898\n",
      "Combination :  116  Training_acc :  0.7914772220736382  Test_acc :  0.7806965410673957\n",
      "Combination :  117  Training_acc :  0.7920715580517667  Test_acc :  0.7766551765125401\n",
      "Combination :  118  Training_acc :  0.8077620278743574  Test_acc :  0.7783192677998336\n",
      "Combination :  119  Training_acc :  0.8214911889691242  Test_acc :  0.7801022227505052\n",
      "Combination :  120  Training_acc :  0.8323081037710618  Test_acc :  0.7808154047307738\n",
      "Combination :  121  Training_acc :  0.8237793824849188  Test_acc :  0.7758231308688934\n",
      "Combination :  122  Training_acc :  0.8523966598318029  Test_acc :  0.7780815404730774\n",
      "Combination :  123  Training_acc :  0.8755757629788119  Test_acc :  0.7782004041364555\n",
      "Combination :  124  Training_acc :  0.8965855398056521  Test_acc :  0.7778438131463211\n",
      "Combination :  125  Training_acc :  0.8811328043743129  Test_acc :  0.7746344942351123\n",
      "Combination :  126  Training_acc :  0.9171495646488961  Test_acc :  0.776536312849162\n",
      "Combination :  127  Training_acc :  0.9434786484799857  Test_acc :  0.7753476762153809\n",
      "Combination :  128  Training_acc :  0.9630323021604112  Test_acc :  0.7747533578984904\n",
      "Combination :  129  Training_acc :  0.7787287153427833  Test_acc :  0.7729704029478188\n",
      "Combination :  130  Training_acc :  0.7918932572583282  Test_acc :  0.7791513134434803\n",
      "Combination :  131  Training_acc :  0.7999465097619685  Test_acc :  0.7801022227505052\n",
      "Combination :  132  Training_acc :  0.8055629847552822  Test_acc :  0.7811719957209081\n",
      "Combination :  133  Training_acc :  0.8069596743038839  Test_acc :  0.7762985855224058\n",
      "Combination :  134  Training_acc :  0.8304359454399572  Test_acc :  0.7783192677998336\n",
      "Combination :  135  Training_acc :  0.8518914742503938  Test_acc :  0.7790324497801022\n",
      "Combination :  136  Training_acc :  0.8693649520073697  Test_acc :  0.777724949482943\n",
      "Combination :  137  Training_acc :  0.8512674214733589  Test_acc :  0.776536312849162\n",
      "Combination :  138  Training_acc :  0.89643695581112  Test_acc :  0.7778438131463211\n",
      "Combination :  139  Training_acc :  0.9291551514070904  Test_acc :  0.7729704029478188\n",
      "Combination :  140  Training_acc :  0.9508781314076847  Test_acc :  0.7736835849280875\n",
      "Combination :  141  Training_acc :  0.9151585391221657  Test_acc :  0.7745156305717342\n",
      "Combination :  142  Training_acc :  0.9652016284805801  Test_acc :  0.7708308570070129\n",
      "Combination :  143  Training_acc :  0.985914237318356  Test_acc :  0.7711874479971472\n",
      "Combination :  144  Training_acc :  0.9945618258001249  Test_acc :  0.7695233567098538\n",
      "Combination :  145  Training_acc :  0.7867225342486107  Test_acc :  0.7757042672055152\n",
      "Combination :  146  Training_acc :  0.7993224569849335  Test_acc :  0.7798644954237489\n",
      "Combination :  147  Training_acc :  0.8081186294612345  Test_acc :  0.7809342683941519\n",
      "Combination :  148  Training_acc :  0.8172416867255059  Test_acc :  0.7812908593842862\n",
      "Combination :  149  Training_acc :  0.8211048705833408  Test_acc :  0.7774872221561868\n",
      "Combination :  150  Training_acc :  0.8537041989836854  Test_acc :  0.778794722453346\n",
      "Combination :  151  Training_acc :  0.8784582924727349  Test_acc :  0.775466539878759\n",
      "Combination :  152  Training_acc :  0.8969421413925291  Test_acc :  0.7770117675026744\n",
      "Combination :  153  Training_acc :  0.8774479213099164  Test_acc :  0.776536312849162\n",
      "Combination :  154  Training_acc :  0.9303438233633473  Test_acc :  0.7699988113633662\n",
      "Combination :  155  Training_acc :  0.96279456776916  Test_acc :  0.7663140377986449\n",
      "Combination :  156  Training_acc :  0.9789605063742534  Test_acc :  0.7673838107690479\n",
      "Combination :  157  Training_acc :  0.9416064901488812  Test_acc :  0.7719006299774159\n",
      "Combination :  158  Training_acc :  0.9879944132418056  Test_acc :  0.7683347200760727\n",
      "Combination :  159  Training_acc :  0.9975632224896734  Test_acc :  0.7685724474028289\n",
      "Combination :  160  Training_acc :  0.9997028320109358  Test_acc :  0.7689290383929632\n",
      "Combination :  161  Training_acc :  0.7185819143561856  Test_acc :  0.7148460715559254\n",
      "Combination :  162  Training_acc :  0.7256247956970076  Test_acc :  0.7220967550219898\n",
      "Combination :  163  Training_acc :  0.737214347270512  Test_acc :  0.7345774396766909\n",
      "Combination :  164  Training_acc :  0.7421176190900716  Test_acc :  0.7396885772019494\n",
      "Combination :  165  Training_acc :  0.7589967608689192  Test_acc :  0.7480090336384168\n",
      "Combination :  166  Training_acc :  0.7630085287212861  Test_acc :  0.753833353143944\n",
      "Combination :  167  Training_acc :  0.7684764197200677  Test_acc :  0.7578747176987994\n",
      "Combination :  168  Training_acc :  0.7740334611155686  Test_acc :  0.765244264828242\n",
      "Combination :  169  Training_acc :  0.7873465870256456  Test_acc :  0.7663140377986449\n",
      "Combination :  170  Training_acc :  0.7917743900627024  Test_acc :  0.7699988113633662\n",
      "Combination :  171  Training_acc :  0.7974205818549226  Test_acc :  0.7752288125520028\n",
      "Combination :  172  Training_acc :  0.8025021544679207  Test_acc :  0.7746344942351123\n",
      "Combination :  173  Training_acc :  0.8201836498172417  Test_acc :  0.7680969927493165\n",
      "Combination :  174  Training_acc :  0.8276425663427536  Test_acc :  0.7708308570070129\n",
      "Combination :  175  Training_acc :  0.8352203500638912  Test_acc :  0.7743967669083561\n",
      "Combination :  176  Training_acc :  0.8431250185729993  Test_acc :  0.7738024485914656\n",
      "Combination :  177  Training_acc :  0.7444949630025853  Test_acc :  0.7419469868061334\n",
      "Combination :  178  Training_acc :  0.7625330599387834  Test_acc :  0.7572803993819089\n",
      "Combination :  179  Training_acc :  0.7725476211702476  Test_acc :  0.7658385831451325\n",
      "Combination :  180  Training_acc :  0.7796796529077887  Test_acc :  0.770949720670391\n",
      "Combination :  181  Training_acc :  0.776945707408398  Test_acc :  0.767502674432426\n",
      "Combination :  182  Training_acc :  0.7906451517042584  Test_acc :  0.7758231308688934\n",
      "Combination :  183  Training_acc :  0.79976820896853  Test_acc :  0.7761797218590277\n",
      "Combination :  184  Training_acc :  0.8080294790645152  Test_acc :  0.7751099488886247\n",
      "Combination :  185  Training_acc :  0.8078511782710767  Test_acc :  0.7757042672055152\n",
      "Combination :  186  Training_acc :  0.8244925856586729  Test_acc :  0.7783192677998336\n",
      "Combination :  187  Training_acc :  0.8393807019107902  Test_acc :  0.7767740401759182\n",
      "Combination :  188  Training_acc :  0.8533773141957148  Test_acc :  0.7782004041364555\n",
      "Combination :  189  Training_acc :  0.8509405366853883  Test_acc :  0.7736835849280875\n",
      "Combination :  190  Training_acc :  0.8784285756738284  Test_acc :  0.7741590395815999\n",
      "Combination :  191  Training_acc :  0.900805325250364  Test_acc :  0.7743967669083561\n",
      "Combination :  192  Training_acc :  0.9175953166324924  Test_acc :  0.7740401759182218\n",
      "Combination :  193  Training_acc :  0.7632759799114439  Test_acc :  0.7579935813621775\n",
      "Combination :  194  Training_acc :  0.7788475825384089  Test_acc :  0.770949720670391\n",
      "Combination :  195  Training_acc :  0.7875843214168969  Test_acc :  0.7771306311660525\n",
      "Combination :  196  Training_acc :  0.792160708448486  Test_acc :  0.7792701771068584\n",
      "Combination :  197  Training_acc :  0.7910909036878547  Test_acc :  0.7741590395815999\n",
      "Combination :  198  Training_acc :  0.8092478678196785  Test_acc :  0.7761797218590277\n",
      "Combination :  199  Training_acc :  0.8220260913494398  Test_acc :  0.7766551765125401\n",
      "Combination :  200  Training_acc :  0.8331401741404416  Test_acc :  0.7758231308688934\n",
      "Combination :  201  Training_acc :  0.8238982496805444  Test_acc :  0.7796267680969927\n",
      "Combination :  202  Training_acc :  0.8532584470000891  Test_acc :  0.7773683584928087\n",
      "Combination :  203  Training_acc :  0.8791417788475825  Test_acc :  0.7736835849280875\n",
      "Combination :  204  Training_acc :  0.897982229354254  Test_acc :  0.774277903244978\n",
      "Combination :  205  Training_acc :  0.8783691420760156  Test_acc :  0.774277903244978\n",
      "Combination :  206  Training_acc :  0.9159608926926391  Test_acc :  0.7741590395815999\n",
      "Combination :  207  Training_acc :  0.9440135508603014  Test_acc :  0.773208130274575\n",
      "Combination :  208  Training_acc :  0.9644289917090131  Test_acc :  0.773208130274575\n",
      "Combination :  209  Training_acc :  0.7797985201034144  Test_acc :  0.7716629026506597\n",
      "Combination :  210  Training_acc :  0.7926956108288015  Test_acc :  0.778794722453346\n",
      "Combination :  211  Training_acc :  0.8003031113488455  Test_acc :  0.7789135861167241\n",
      "Combination :  212  Training_acc :  0.8065436391191941  Test_acc :  0.7803399500772614\n",
      "Combination :  213  Training_acc :  0.8088021158360822  Test_acc :  0.7771306311660525\n",
      "Combination :  214  Training_acc :  0.8338533773141957  Test_acc :  0.776536312849162\n",
      "Combination :  215  Training_acc :  0.8555763573147901  Test_acc :  0.7743967669083561\n",
      "Combination :  216  Training_acc :  0.8729309678761403  Test_acc :  0.7743967669083561\n",
      "Combination :  217  Training_acc :  0.8529909958099313  Test_acc :  0.7751099488886247\n",
      "Combination :  218  Training_acc :  0.9017265460164631  Test_acc :  0.7748722215618685\n",
      "Combination :  219  Training_acc :  0.9330480520638317  Test_acc :  0.7735647212647094\n",
      "Combination :  220  Training_acc :  0.9559299872217765  Test_acc :  0.7719006299774159\n",
      "Combination :  221  Training_acc :  0.9202401117351638  Test_acc :  0.7726138119576845\n",
      "Combination :  222  Training_acc :  0.96885679474607  Test_acc :  0.7715440389872816\n",
      "Combination :  223  Training_acc :  0.988232147633057  Test_acc :  0.7724949482943064\n",
      "Combination :  224  Training_acc :  0.9964339841312294  Test_acc :  0.7715440389872816\n",
      "Combination :  225  Training_acc :  0.787940923003774  Test_acc :  0.7773683584928087\n",
      "Combination :  226  Training_acc :  0.7997384921696234  Test_acc :  0.7803399500772614\n",
      "Combination :  227  Training_acc :  0.8087426822382693  Test_acc :  0.7782004041364555\n",
      "Combination :  228  Training_acc :  0.8167959347419096  Test_acc :  0.7783192677998336\n",
      "Combination :  229  Training_acc :  0.8222341089417848  Test_acc :  0.7797456317603708\n",
      "Combination :  230  Training_acc :  0.8559626757005735  Test_acc :  0.776536312849162\n",
      "Combination :  231  Training_acc :  0.8819945915425991  Test_acc :  0.7741590395815999\n",
      "Combination :  232  Training_acc :  0.9031529523639713  Test_acc :  0.7729704029478188\n",
      "Combination :  233  Training_acc :  0.8810139371786871  Test_acc :  0.7749910852252466\n",
      "Combination :  234  Training_acc :  0.9356631303675969  Test_acc :  0.773208130274575\n",
      "Combination :  235  Training_acc :  0.9649638940893287  Test_acc :  0.7702365386901224\n",
      "Combination :  236  Training_acc :  0.982675106237556  Test_acc :  0.768691311066207\n",
      "Combination :  237  Training_acc :  0.9491842738700187  Test_acc :  0.7710685843337691\n",
      "Combination :  238  Training_acc :  0.9884401652254019  Test_acc :  0.7699988113633662\n",
      "Combination :  239  Training_acc :  0.9979198240765504  Test_acc :  0.7689290383929632\n",
      "Combination :  240  Training_acc :  0.9997325488098422  Test_acc :  0.7677404017591822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'n_estimators': [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "result = {\n",
    "    'index':[],\n",
    "    'learning_rate' : [],\n",
    "    'max_depth': [],\n",
    "    'colsample_bytree': [],\n",
    "    'n_estimators': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "\n",
    "x_train_transformed = preprocessor.fit_transform(x_train)\n",
    "x_test_transformed = preprocessor.transform(x_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_preprocessed = encoder.fit_transform(y_train)\n",
    "y_test_preprocessed = encoder.transform(y_test)\n",
    "\n",
    "index = 1\n",
    "\n",
    "for i in param_grid['colsample_bytree']:\n",
    "    for j in param_grid['learning_rate']:\n",
    "        for k in param_grid['max_depth']:\n",
    "                for l in param_grid['n_estimators']:\n",
    "\n",
    "                    xgboost = XGBClassifier(colsample_bytree=i,learning_rate=j,max_depth=k,n_estimators=l)\n",
    "                    xgboost.fit(x_train_transformed,y_train_preprocessed)\n",
    "\n",
    "                    train_pred = xgboost.predict(x_train_transformed)\n",
    "                    test_pred = xgboost.predict(x_test_transformed)\n",
    "\n",
    "                    train_accuracy = accuracy_score(y_train_preprocessed, train_pred)\n",
    "                    test_accuracy = accuracy_score(y_test_preprocessed, test_pred)\n",
    "\n",
    "                    result['index'].append(index)\n",
    "                    result['colsample_bytree'].append(i)\n",
    "                    result['learning_rate'].append(j)\n",
    "                    result['max_depth'].append(k)                    \n",
    "                    result['n_estimators'].append(l)\n",
    "                    result['train_acc'].append(train_accuracy)\n",
    "                    result['test_acc'].append(test_accuracy)\n",
    "\n",
    "                    print(\"Combination : \" ,index, \" Training_acc : \" , train_accuracy, \" Test_acc : \", test_accuracy)\n",
    "\n",
    "                    index+=1\n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>100</td>\n",
       "      <td>0.799114</td>\n",
       "      <td>0.782479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.804761</td>\n",
       "      <td>0.782004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>150</td>\n",
       "      <td>0.799412</td>\n",
       "      <td>0.781766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>150</td>\n",
       "      <td>0.806930</td>\n",
       "      <td>0.781647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.828861</td>\n",
       "      <td>0.781291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>200</td>\n",
       "      <td>0.817242</td>\n",
       "      <td>0.781291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>132</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>200</td>\n",
       "      <td>0.805563</td>\n",
       "      <td>0.781172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>100</td>\n",
       "      <td>0.828237</td>\n",
       "      <td>0.780934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.814003</td>\n",
       "      <td>0.780934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>150</td>\n",
       "      <td>0.808119</td>\n",
       "      <td>0.780934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  learning_rate  max_depth  colsample_bytree  n_estimators  \\\n",
       "65      66            0.3          3               0.6           100   \n",
       "51      52            0.2          3               0.6           200   \n",
       "50      51            0.2          3               0.6           150   \n",
       "66      67            0.3          3               0.6           150   \n",
       "39      40            0.1          5               0.6           200   \n",
       "147    148            0.3          3               0.8           200   \n",
       "131    132            0.2          3               0.8           200   \n",
       "53      54            0.2          5               0.6           100   \n",
       "67      68            0.3          3               0.6           200   \n",
       "146    147            0.3          3               0.8           150   \n",
       "\n",
       "     train_acc  test_acc  \n",
       "65    0.799114  0.782479  \n",
       "51    0.804761  0.782004  \n",
       "50    0.799412  0.781766  \n",
       "66    0.806930  0.781647  \n",
       "39    0.828861  0.781291  \n",
       "147   0.817242  0.781291  \n",
       "131   0.805563  0.781172  \n",
       "53    0.828237  0.780934  \n",
       "67    0.814003  0.780934  \n",
       "146   0.808119  0.780934  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result)\n",
    "\n",
    "\n",
    "sorted_results_df = results_df.sort_values(by='test_acc', ascending=False)\n",
    "\n",
    "\n",
    "sorted_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following results highlight the optimal parameters found during the hyperparameter tuning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
